{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3d86f08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#preprecess imports\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "import en_core_web_sm\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "nlp = en_core_web_sm.load()\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "# Essentials\n",
    "import base64\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Gensim and LDA\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models  # don't skip this \n",
    "\n",
    "# NLP stuff\n",
    "import contractions\n",
    "import demoji\n",
    "import string\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "# Plotting tools\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#sklearn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0716fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('practice.csv', usecols = ['position_section','hl1', 'hl2', 'lede', 'body'])\n",
    "df2 = pd.read_csv('practice2.csv', usecols = ['position_section','hl1', 'hl2', 'lede', 'body'])\n",
    "df3 = pd.read_csv('practice3.csv', usecols = ['position_section','hl1', 'hl2', 'lede', 'body'])\n",
    "df4 = pd.read_csv('practice4.csv', usecols = ['position_section','hl1', 'hl2', 'lede', 'body'])\n",
    "df5 = pd.read_csv('practice5.csv', usecols = ['position_section','hl1', 'hl2', 'lede', 'body'])\n",
    "\n",
    "li = []\n",
    "\n",
    "li.append(df)\n",
    "li.append(df2)\n",
    "li.append(df3)\n",
    "li.append(df4)\n",
    "li.append(df5)\n",
    "    \n",
    "frame = pd.concat(li, axis = 0, ignore_index = True)\n",
    "\n",
    "df = frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07e2c7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis='rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99d194f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position_section</th>\n",
       "      <th>hl1</th>\n",
       "      <th>hl2</th>\n",
       "      <th>lede</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Wilfork goes out a legend</td>\n",
       "      <td>Lineman leaves Pats for Texans</td>\n",
       "      <td>Vince Wilfork will be immortalized as a Patrio...</td>\n",
       "      <td>'Few players reached or will ever reach the sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OPINION</td>\n",
       "      <td>Hillary's modus operandi not pretty</td>\n",
       "      <td>'Nasty' GOP latest excuse for private email use</td>\n",
       "      <td>Just when you thought we were done with emailg...</td>\n",
       "      <td>Didn't Clinton tell us that she set up the pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>RETAILERS FEAR TRASHED STREETS</td>\n",
       "      <td>Mess left beneath snow piles could slow return...</td>\n",
       "      <td>With dirty snowbanks still lining many streets...</td>\n",
       "      <td>'After this very tough winter for main streets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FEATURES</td>\n",
       "      <td>Night of the laughing dead</td>\n",
       "      <td>CW's 'iZombie' mixes horror, mystery, comedy</td>\n",
       "      <td>It's the 'Walking Dead'  'Quincy'/'Psych' mash...</td>\n",
       "      <td>One night, the super-overachiever decides to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Sox see what's missing</td>\n",
       "      <td>Harvey shows off top-of-rotation arm</td>\n",
       "      <td>FORT MYERS - It's not just the results, the co...</td>\n",
       "      <td>The Red Sox don't have a Matt Harvey, and they...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  position_section                                  hl1  \\\n",
       "1           SPORTS            Wilfork goes out a legend   \n",
       "2          OPINION  Hillary's modus operandi not pretty   \n",
       "6         BUSINESS       RETAILERS FEAR TRASHED STREETS   \n",
       "7         FEATURES           Night of the laughing dead   \n",
       "8           SPORTS               Sox see what's missing   \n",
       "\n",
       "                                                 hl2  \\\n",
       "1                     Lineman leaves Pats for Texans   \n",
       "2    'Nasty' GOP latest excuse for private email use   \n",
       "6  Mess left beneath snow piles could slow return...   \n",
       "7       CW's 'iZombie' mixes horror, mystery, comedy   \n",
       "8               Harvey shows off top-of-rotation arm   \n",
       "\n",
       "                                                lede  \\\n",
       "1  Vince Wilfork will be immortalized as a Patrio...   \n",
       "2  Just when you thought we were done with emailg...   \n",
       "6  With dirty snowbanks still lining many streets...   \n",
       "7  It's the 'Walking Dead'  'Quincy'/'Psych' mash...   \n",
       "8  FORT MYERS - It's not just the results, the co...   \n",
       "\n",
       "                                                body  \n",
       "1  'Few players reached or will ever reach the sp...  \n",
       "2  Didn't Clinton tell us that she set up the pri...  \n",
       "6  'After this very tough winter for main streets...  \n",
       "7  One night, the super-overachiever decides to t...  \n",
       "8  The Red Sox don't have a Matt Harvey, and they...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6cd960",
   "metadata": {},
   "source": [
    "### Importing with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95990ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e324cdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(\"/projectnb/sparkgrp/ds-naacp-media-bias/TBG_unique_raw.csv\", usecols = ['position_section', 'hl1', 'hl2', 'lede', 'body'])\n",
    "df = df.map_partitions(lambda x: x.fillna(''))\n",
    "df = df.map_partitions(lambda x: x[['hl1', 'hl2', 'lede', 'body']].agg(' '.join, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0a2f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.partitions[5].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27700cd3",
   "metadata": {},
   "source": [
    "### Preprocessing with Badnani's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ed888f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:04<00:00, 52.16it/s]\n"
     ]
    }
   ],
   "source": [
    "#/projectnb/sparkgrp/ds-naacp-media-bias\n",
    "# def combine_led_body(df):\n",
    "#     body = df['body'].values\n",
    "#     lede = df['lede'].values\n",
    "#     arr = np.vstack((lede,body)).T\n",
    "#     for step, x in enumerate(arr): \n",
    "#         arr[step] = ' '.join(x)\n",
    "\n",
    "#     arr = arr.flatten()\n",
    "#     return arr \n",
    "\n",
    "\n",
    "df = pd.read_csv('practice.csv', usecols = ['position_section','hl1', 'hl2', 'lede', 'body'])\n",
    "df2 = pd.read_csv('practice2.csv', usecols = ['position_section','hl1', 'hl2', 'lede', 'body'])\n",
    "df3 = pd.read_csv('practice3.csv', usecols = ['position_section','hl1', 'hl2', 'lede', 'body'])\n",
    "df4 = pd.read_csv('practice4.csv', usecols = ['position_section','hl1', 'hl2', 'lede', 'body'])\n",
    "df5 = pd.read_csv('practice5.csv', usecols = ['position_section','hl1', 'hl2', 'lede', 'body'])\n",
    "\n",
    "li = []\n",
    "\n",
    "li.append(df)\n",
    "li.append(df2)\n",
    "li.append(df3)\n",
    "li.append(df4)\n",
    "li.append(df5)\n",
    "    \n",
    "frame = pd.concat(li)\n",
    "\n",
    "df = frame\n",
    "df.fillna('', inplace = True)\n",
    "df['content'] = df[['hl1', 'hl2', 'lede', 'body']].agg(' '.join, axis=1)\n",
    "df.drop(columns = ['hl1', 'hl2', 'lede', 'body'], inplace = True)\n",
    "\n",
    "def custom_preprocess(corpus_iterable):\n",
    "    return [['<s>'] + [z.lemma_.lower() for z in y if not (z.is_stop or z.text == '\\n')] + ['</s>']\n",
    "            for x in tqdm(nlp.pipe(corpus_iterable), total = len(corpus_iterable))\n",
    "            for y in x.sents]\n",
    "\n",
    "with nlp.select_pipes(disable=['tok2vec', 'parser', 'ner']):\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "    corpus = []\n",
    "    temp = []\n",
    "\n",
    "    for x in tqdm(nlp.pipe(df.content.values), total = len(df.content.values)):\n",
    "        for y in x.sents:\n",
    "            temp.append('<s>')\n",
    "            for z in y:\n",
    "                if not (z.is_stop or z.text == '\\n'):\n",
    "                    temp.append(z)\n",
    "            temp.append('</s>')\n",
    "        corpus.append(temp)\n",
    "        temp = []\n",
    "    #corpus = custom_preprocess(df.content.values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d46db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = combine_led_body(df)\n",
    "df.content.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "406ba51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63c4bd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.to_csv('preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b4806f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus) #each body+lede combination is represented twice in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcd4ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a6e734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_preprocess(corpus_iterable): \n",
    "    return [[z.lemma_.lower() for z in y if not (z.is_stop or z.text == '\\n')] for y in \n",
    "tqdm(nlp.pipe(corpus_iterable), total = len(corpus_iterable)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0ed718",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_corpus = custom_preprocess(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd2c7c4",
   "metadata": {},
   "source": [
    "## TDS LDA Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989bf5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text_col):\n",
    "    \"\"\"This function will apply NLP preprocessing lambda functions over a pandas series such as df['text'].\n",
    "       These functions include converting text to lowercase, removing emojis, expanding contractions, removing punctuation,\n",
    "       removing numbers, removing stopwords, lemmatization, etc.\"\"\"\n",
    "    \n",
    "    # convert to lowercase\n",
    "    text_col = text_col.apply(lambda x: ' '.join([w.lower() for w in x.split()]))\n",
    "    \n",
    "    # remove emojis\n",
    "    text_col = text_col.apply(lambda x: demoji.replace(x, \"\"))\n",
    "    \n",
    "    # expand contractions  \n",
    "    text_col = text_col.apply(lambda x: ' '.join([contractions.fix(word) for word in x.split()]))\n",
    "\n",
    "    # remove punctuation\n",
    "    text_col = text_col.apply(lambda x: ''.join([i for i in x if i not in string.punctuation]))\n",
    "    \n",
    "    # remove numbers\n",
    "    text_col = text_col.apply(lambda x: ' '.join(re.sub(\"[^a-zA-Z]+\", \" \", x).split()))\n",
    "\n",
    "    # remove stopwords\n",
    "    stopwords = [sw for sw in nltk.corpus.stopwords.words('english') if sw not in ['not', 'no']]\n",
    "    text_col = text_col.apply(lambda x: ' '.join([w for w in x.split() if w not in stopwords]))\n",
    "\n",
    "    # lemmatization\n",
    "    text_col = text_col.apply(lambda x: ' '.join([WordNetLemmatizer().lemmatize(w) for w in x.split()]))\n",
    "\n",
    "    # remove short words\n",
    "    text_col = text_col.apply(lambda x: ' '.join([w.strip() for w in x.split() if len(w.strip()) >= 3]))\n",
    "\n",
    "    return text_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3517e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocess(df['body']))\n",
    "preprocessed_df = preprocess(df['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aab9208",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessed_df.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae93dbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['body'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aa0fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc823c6",
   "metadata": {},
   "source": [
    "## Working LDA Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1565c7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gd/pxcx2w0x59j2bsjbzhy62nvw0000gn/T/ipykernel_27028/376646926.py:6: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  documents = pd.read_csv('practice.csv', error_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "# Use CountVectorizor to find three letter tokens, remove stop_words, \n",
    "# remove tokens that don't appear in at least 20 documents,\n",
    "# remove tokens that appear in more than 20% of the documents\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "documents = pd.read_csv('practice.csv', error_bad_lines=False)\n",
    "documents = documents.dropna(axis='rows')\n",
    "documents = df\n",
    "documents.head()\n",
    "\n",
    "\n",
    "vect = CountVectorizer(stop_words='english', \n",
    "                       token_pattern='(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b')\n",
    "# Fit and transform\n",
    "X = vect.fit_transform(documents.content)\n",
    "# Convert sparse matrix to gensim corpus.\n",
    "corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n",
    "# Mapping from word IDs to words (To be used in LdaModel's id2word parameter)\n",
    "id_map = dict((v, k) for k, v in vect.vocabulary_.items())\n",
    "# Use the gensim.models.ldamodel.LdaModel constructor to estimate \n",
    "# LDA model parameters on the corpus, and save to the variable `ldamodel`\n",
    "ldamodel = gensim.models.LdaMulticore(corpus=corpus, id2word=id_map, passes=30,\n",
    "                                               random_state=42, num_topics=20, workers=2, per_word_topics = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b6f5d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.015*\"said\" + 0.005*\"trump\" + 0.005*\"mullin\" + 0.004*\"officer\" + 0.004*\"navy\" + 0.004*\"school\" + 0.004*\"gallagher\" + 0.004*\"president\" + 0.004*\"companies\" + 0.004*\"saunders\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.015*\"said\" + 0.005*\"new\" + 0.005*\"clark\" + 0.005*\"students\" + 0.004*\"weinstein\" + 0.003*\"people\" + 0.003*\"week\" + 0.003*\"like\" + 0.003*\"acting\" + 0.003*\"woman\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.011*\"trump\" + 0.010*\"said\" + 0.005*\"trade\" + 0.005*\"new\" + 0.005*\"state\" + 0.004*\"deal\" + 0.004*\"image\" + 0.003*\"link\" + 0.003*\"000\" + 0.003*\"walsh\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.015*\"said\" + 0.007*\"artists\" + 0.006*\"day\" + 0.005*\"booster\" + 0.005*\"new\" + 0.004*\"dose\" + 0.004*\"space\" + 0.004*\"building\" + 0.004*\"moderna\" + 0.004*\"office\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.018*\"said\" + 0.005*\"long\" + 0.004*\"boston\" + 0.004*\"like\" + 0.003*\"time\" + 0.003*\"year\" + 0.003*\"people\" + 0.003*\"government\" + 0.003*\"new\" + 0.003*\"saturday\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.011*\"said\" + 0.010*\"state\" + 0.005*\"states\" + 0.005*\"baker\" + 0.005*\"family\" + 0.004*\"settlement\" + 0.004*\"million\" + 0.004*\"texas\" + 0.004*\"wednesday\" + 0.004*\"new\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.015*\"said\" + 0.007*\"year\" + 0.006*\"laws\" + 0.005*\"named\" + 0.005*\"victims\" + 0.004*\"state\" + 0.004*\"marijuana\" + 0.004*\"image\" + 0.004*\"percent\" + 0.004*\"link\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.010*\"said\" + 0.009*\"abortion\" + 0.008*\"law\" + 0.005*\"texas\" + 0.004*\"people\" + 0.004*\"refugee\" + 0.004*\"chancellor\" + 0.004*\"merkel\" + 0.004*\"car\" + 0.003*\"included\"\n",
      "\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.016*\"game\" + 0.013*\"yards\" + 0.013*\"season\" + 0.012*\"link\" + 0.012*\"image\" + 0.010*\"said\" + 0.008*\"team\" + 0.008*\"games\" + 0.008*\"state\" + 0.006*\"com\"\n",
      "\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.015*\"said\" + 0.005*\"state\" + 0.004*\"year\" + 0.004*\"netanyahu\" + 0.004*\"republican\" + 0.003*\"ballot\" + 0.003*\"mask\" + 0.003*\"arizona\" + 0.003*\"school\" + 0.003*\"ducey\"\n",
      "\n",
      "\n",
      "Topic: 10 \n",
      "Words: 0.008*\"best\" + 0.008*\"google\" + 0.007*\"gun\" + 0.007*\"said\" + 0.006*\"sanders\" + 0.005*\"new\" + 0.005*\"company\" + 0.005*\"warren\" + 0.005*\"background\" + 0.005*\"link\"\n",
      "\n",
      "\n",
      "Topic: 11 \n",
      "Words: 0.010*\"said\" + 0.008*\"statement\" + 0.007*\"memphis\" + 0.006*\"link\" + 0.006*\"image\" + 0.006*\"season\" + 0.006*\"team\" + 0.006*\"heart\" + 0.006*\"failure\" + 0.006*\"ole\"\n",
      "\n",
      "\n",
      "Topic: 12 \n",
      "Words: 0.009*\"law\" + 0.007*\"said\" + 0.006*\"state\" + 0.006*\"brien\" + 0.005*\"school\" + 0.005*\"best\" + 0.005*\"conference\" + 0.005*\"link\" + 0.005*\"image\" + 0.005*\"harvard\"\n",
      "\n",
      "\n",
      "Topic: 13 \n",
      "Words: 0.015*\"children\" + 0.010*\"vaccines\" + 0.007*\"said\" + 0.005*\"infections\" + 0.005*\"vaccine\" + 0.005*\"delta\" + 0.005*\"younger\" + 0.005*\"vaccinated\" + 0.005*\"variant\" + 0.005*\"cases\"\n",
      "\n",
      "\n",
      "Topic: 14 \n",
      "Words: 0.017*\"said\" + 0.009*\"mcclain\" + 0.009*\"police\" + 0.006*\"year\" + 0.006*\"officers\" + 0.005*\"general\" + 0.004*\"officer\" + 0.004*\"investigation\" + 0.004*\"fbi\" + 0.004*\"department\"\n",
      "\n",
      "\n",
      "Topic: 15 \n",
      "Words: 0.016*\"boston\" + 0.010*\"smart\" + 0.009*\"celtics\" + 0.008*\"city\" + 0.007*\"irving\" + 0.006*\"new\" + 0.006*\"hayward\" + 0.005*\"fan\" + 0.005*\"want\" + 0.004*\"told\"\n",
      "\n",
      "\n",
      "Topic: 16 \n",
      "Words: 0.012*\"said\" + 0.009*\"image\" + 0.008*\"link\" + 0.005*\"police\" + 0.004*\"iran\" + 0.004*\"year\" + 0.004*\"tuesday\" + 0.004*\"says\" + 0.004*\"people\" + 0.003*\"clinton\"\n",
      "\n",
      "\n",
      "Topic: 17 \n",
      "Words: 0.042*\"president\" + 0.021*\"trump\" + 0.012*\"house\" + 0.011*\"ukraine\" + 0.011*\"ambassador\" + 0.008*\"zelensky\" + 0.007*\"white\" + 0.007*\"impeachment\" + 0.006*\"sondland\" + 0.006*\"investigations\"\n",
      "\n",
      "\n",
      "Topic: 18 \n",
      "Words: 0.016*\"said\" + 0.015*\"game\" + 0.009*\"points\" + 0.008*\"just\" + 0.006*\"link\" + 0.006*\"image\" + 0.006*\"thanksgiving\" + 0.005*\"second\" + 0.005*\"celtics\" + 0.005*\"high\"\n",
      "\n",
      "\n",
      "Topic: 19 \n",
      "Words: 0.014*\"said\" + 0.012*\"team\" + 0.009*\"game\" + 0.007*\"just\" + 0.006*\"games\" + 0.005*\"season\" + 0.005*\"play\" + 0.005*\"going\" + 0.005*\"don\" + 0.004*\"time\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in ldamodel.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db25a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e38078",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(df.body.tolist() ).lower() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5f3295",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the df to one candidate, and create a list of responses from them\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for i in df.body.values:\n",
    "    text = i\n",
    "\n",
    "    # join the list and lowercase all the words\n",
    "    #text = ' '.join([text]).lower()\n",
    "\n",
    "    #create the wordcloud object\n",
    "    wordcloud = WordCloud(stopwords = STOPWORDS,\n",
    "                          collocations=True).generate(text)\n",
    "\n",
    "    #plot the wordcloud object\n",
    "    plt.imshow(wordcloud, interpolation='bilInear')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ee4e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def topic_distribution(string_input):\n",
    "    string_input = [string_input]\n",
    "    # Fit and transform\n",
    "    X = vect.transform(string_input)\n",
    "    # Convert sparse matrix to gensim corpus.\n",
    "    corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n",
    "    output = list(ldamodel[corpus])[0]\n",
    "    return output\n",
    " \n",
    "topic_distribution(documents.body[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b301540e",
   "metadata": {},
   "source": [
    "## Second Approach to topic modeling using PAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eae3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as tp\n",
    "mdl = tp.PAModel(k1=5, k2=25)\n",
    "for docs in df.content.values:\n",
    "    mdl.add_doc(docs)\n",
    "mdl.train(1000)\n",
    "\n",
    "# first, infer the document\n",
    "doc_inst = mdl.make_doc(df.content.values[0])\n",
    "mdl.infer(doc_inst)\n",
    "\n",
    "# next, count subtopics\n",
    "subtopic_counts = [0] * mdl.k2\n",
    "for t in doc_inst.subtopics:\n",
    "    subtopic_counts[t] += 1\n",
    "\n",
    "# estimate distribution\n",
    "total = sum(subtopic_counts)\n",
    "for i in range(mdl.k2):\n",
    "    print('Subtopic #{} : {}'.format(i, subtopic_counts[i] / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f3e3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.content.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04bb136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently struggling to make sense of subtopic outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3263ff6b",
   "metadata": {},
   "source": [
    "## Automatic labeling with Tomotopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb615461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4367352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = tp.utils.Corpus(tokenizer=tp.utils.SimpleTokenizer(), stopwords=['.'])\n",
    "# data_feeder yields a tuple of (raw string, user data) or a str (raw string)\n",
    "corpus.process(open(\"preprocessed.csv\", encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b01e65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num docs: 257 , Vocab size: 1961 , Num words: 129479\n",
      "Removed top words: []\n",
      "Iteration: 0\tLog-likelihood: -7.3253967857652516\n",
      "Iteration: 10\tLog-likelihood: -7.07386861610103\n",
      "Iteration: 20\tLog-likelihood: -6.967255645934394\n",
      "Iteration: 30\tLog-likelihood: -6.91065479245327\n",
      "Iteration: 40\tLog-likelihood: -6.869271837660496\n",
      "Iteration: 50\tLog-likelihood: -6.83428178643925\n",
      "Iteration: 60\tLog-likelihood: -6.818519522383362\n",
      "Iteration: 70\tLog-likelihood: -6.797362179659312\n",
      "Iteration: 80\tLog-likelihood: -6.780528986881967\n",
      "Iteration: 90\tLog-likelihood: -6.755442248524666\n",
      "Iteration: 100\tLog-likelihood: -6.74060331267081\n",
      "Iteration: 110\tLog-likelihood: -6.736824803320526\n",
      "Iteration: 120\tLog-likelihood: -6.728048811210432\n",
      "Iteration: 130\tLog-likelihood: -6.7105131141227945\n",
      "Iteration: 140\tLog-likelihood: -6.703054249220132\n",
      "Iteration: 150\tLog-likelihood: -6.697962539381938\n",
      "Iteration: 160\tLog-likelihood: -6.679774014627596\n",
      "Iteration: 170\tLog-likelihood: -6.664726606834184\n",
      "Iteration: 180\tLog-likelihood: -6.6732012959391325\n",
      "Iteration: 190\tLog-likelihood: -6.66877506068009\n",
      "Iteration: 200\tLog-likelihood: -6.667395180607694\n",
      "Iteration: 210\tLog-likelihood: -6.666433657844082\n",
      "Iteration: 220\tLog-likelihood: -6.652811555388541\n",
      "Iteration: 230\tLog-likelihood: -6.652537048751467\n",
      "Iteration: 240\tLog-likelihood: -6.640740478849117\n",
      "Iteration: 250\tLog-likelihood: -6.64346733133491\n",
      "Iteration: 260\tLog-likelihood: -6.642259960713129\n",
      "Iteration: 270\tLog-likelihood: -6.6461570142041495\n",
      "Iteration: 280\tLog-likelihood: -6.640874525390039\n",
      "Iteration: 290\tLog-likelihood: -6.631258502560225\n",
      "Iteration: 300\tLog-likelihood: -6.6199523175702355\n",
      "Iteration: 310\tLog-likelihood: -6.62595722882718\n",
      "Iteration: 320\tLog-likelihood: -6.6140860502699015\n",
      "Iteration: 330\tLog-likelihood: -6.603555570893098\n",
      "Iteration: 340\tLog-likelihood: -6.593301197753301\n",
      "Iteration: 350\tLog-likelihood: -6.5961390902303085\n",
      "Iteration: 360\tLog-likelihood: -6.5919788419283805\n",
      "Iteration: 370\tLog-likelihood: -6.590591127952944\n",
      "Iteration: 380\tLog-likelihood: -6.591970549808794\n",
      "Iteration: 390\tLog-likelihood: -6.593894294192985\n",
      "Iteration: 400\tLog-likelihood: -6.5873673802123305\n",
      "Iteration: 410\tLog-likelihood: -6.58199462505795\n",
      "Iteration: 420\tLog-likelihood: -6.581416721377861\n",
      "Iteration: 430\tLog-likelihood: -6.570096031382942\n",
      "Iteration: 440\tLog-likelihood: -6.568904865483195\n",
      "Iteration: 450\tLog-likelihood: -6.566380546922793\n",
      "Iteration: 460\tLog-likelihood: -6.56224588539757\n",
      "Iteration: 470\tLog-likelihood: -6.566234968434287\n",
      "Iteration: 480\tLog-likelihood: -6.5606825955792205\n",
      "Iteration: 490\tLog-likelihood: -6.561376746401682\n",
      "Iteration: 500\tLog-likelihood: -6.554808466750713\n",
      "Iteration: 510\tLog-likelihood: -6.5585422655707575\n",
      "Iteration: 520\tLog-likelihood: -6.551733130057664\n",
      "Iteration: 530\tLog-likelihood: -6.550461218244053\n",
      "Iteration: 540\tLog-likelihood: -6.554763829859742\n",
      "Iteration: 550\tLog-likelihood: -6.551700565592056\n",
      "Iteration: 560\tLog-likelihood: -6.547520658123855\n",
      "Iteration: 570\tLog-likelihood: -6.54337704852079\n",
      "Iteration: 580\tLog-likelihood: -6.540959385710592\n",
      "Iteration: 590\tLog-likelihood: -6.542287446225001\n",
      "Iteration: 600\tLog-likelihood: -6.536391244905273\n",
      "Iteration: 610\tLog-likelihood: -6.5320415244580525\n",
      "Iteration: 620\tLog-likelihood: -6.525164029400489\n",
      "Iteration: 630\tLog-likelihood: -6.531496524961107\n",
      "Iteration: 640\tLog-likelihood: -6.53179232708804\n",
      "Iteration: 650\tLog-likelihood: -6.539023897919372\n",
      "Iteration: 660\tLog-likelihood: -6.53288406508489\n",
      "Iteration: 670\tLog-likelihood: -6.527004724777971\n",
      "Iteration: 680\tLog-likelihood: -6.524108030313038\n",
      "Iteration: 690\tLog-likelihood: -6.5246947422319925\n",
      "Iteration: 700\tLog-likelihood: -6.524675418944032\n",
      "Iteration: 710\tLog-likelihood: -6.5230685892252\n",
      "Iteration: 720\tLog-likelihood: -6.525446954277499\n",
      "Iteration: 730\tLog-likelihood: -6.522992891727005\n",
      "Iteration: 740\tLog-likelihood: -6.523370848180735\n",
      "Iteration: 750\tLog-likelihood: -6.5253077499544405\n",
      "Iteration: 760\tLog-likelihood: -6.53064114958572\n",
      "Iteration: 770\tLog-likelihood: -6.525325841652694\n",
      "Iteration: 780\tLog-likelihood: -6.52777429650036\n",
      "Iteration: 790\tLog-likelihood: -6.524602737025594\n",
      "Iteration: 800\tLog-likelihood: -6.524782325453035\n",
      "Iteration: 810\tLog-likelihood: -6.520838592619606\n",
      "Iteration: 820\tLog-likelihood: -6.51679188817472\n",
      "Iteration: 830\tLog-likelihood: -6.511176203336697\n",
      "Iteration: 840\tLog-likelihood: -6.514588989555932\n",
      "Iteration: 850\tLog-likelihood: -6.506778760226521\n",
      "Iteration: 860\tLog-likelihood: -6.514309552021053\n",
      "Iteration: 870\tLog-likelihood: -6.515498692544519\n",
      "Iteration: 880\tLog-likelihood: -6.50668543450042\n",
      "Iteration: 890\tLog-likelihood: -6.5055639213858445\n",
      "Iteration: 900\tLog-likelihood: -6.505168062085671\n",
      "Iteration: 910\tLog-likelihood: -6.506325019450005\n",
      "Iteration: 920\tLog-likelihood: -6.498296804199953\n",
      "Iteration: 930\tLog-likelihood: -6.498613027961101\n",
      "Iteration: 940\tLog-likelihood: -6.501729271974922\n",
      "Iteration: 950\tLog-likelihood: -6.4980694440425175\n",
      "Iteration: 960\tLog-likelihood: -6.500707112809784\n",
      "Iteration: 970\tLog-likelihood: -6.498565796550746\n",
      "Iteration: 980\tLog-likelihood: -6.499799919456446\n",
      "Iteration: 990\tLog-likelihood: -6.491868031002641\n"
     ]
    }
   ],
   "source": [
    "# make LDA model and train\n",
    "mdl = tp.LDAModel(k=10, min_cf=10, min_df=5, corpus=corpus)\n",
    "mdl.train(0)\n",
    "print('Num docs:', len(mdl.docs), ', Vocab size:', len(mdl.used_vocabs), ', Num words:', mdl.num_words)\n",
    "print('Removed top words:', mdl.removed_top_words)\n",
    "for i in range(0, 1000, 10):\n",
    "    mdl.train(10)\n",
    "    print('Iteration: {}\\tLog-likelihood: {}'.format(i, mdl.ll_per_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45b982ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract candidates for auto topic labeling\n",
    "extractor = tp.label.PMIExtractor(min_cf=10, min_df=5, max_len=5, max_cand=10000)\n",
    "cands = extractor.extract(mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2fb2bc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Topic #0 ==\n",
      "Labels: 2016 election, military aid, the Intelligence, the President and, Biden and\n",
      "the\t0.08113724738359451\n",
      "to\t0.05791759490966797\n",
      "president\t0.049000538885593414\n",
      "trump\t0.03240245580673218\n",
      "s\t0.02639889530837536\n",
      "that\t0.026134032756090164\n",
      "of\t0.025692595168948174\n",
      "and\t0.01862958073616028\n",
      "his\t0.015980949625372887\n",
      "house\t0.014038621447980404\n",
      "\n",
      "== Topic #1 ==\n",
      "Labels: last season, a game, this season, starters, game and\n",
      "the\t0.06602651625871658\n",
      "to\t0.03563389554619789\n",
      "s\t0.032722726464271545\n",
      "for\t0.02573591284453869\n",
      "team\t0.02131093293428421\n",
      "game\t0.018981996923685074\n",
      "season\t0.016653060913085938\n",
      "games\t0.015837932005524635\n",
      "we\t0.015721485018730164\n",
      "a\t0.015022804960608482\n",
      "\n",
      "== Topic #2 ==\n",
      "Labels: advocates, groups, laws, Legislature, Democrat\n",
      "law\t0.031009608879685402\n",
      "school\t0.028754569590091705\n",
      "students\t0.027908930554986\n",
      "that\t0.02621765062212944\n",
      "state\t0.02283509261906147\n",
      "said\t0.018888773396611214\n",
      "for\t0.018888773396611214\n",
      "laws\t0.01606997288763523\n",
      "university\t0.014942453242838383\n",
      "an\t0.013251174241304398\n",
      "\n",
      "== Topic #3 ==\n",
      "Labels: No. 1, starters, this season, a game, various\n",
      "the\t0.15564283728599548\n",
      "and\t0.084975466132164\n",
      "a\t0.07084711641073227\n",
      "of\t0.06597527861595154\n",
      "in\t0.05435977503657341\n",
      "to\t0.026051798835396767\n",
      "with\t0.02392357401549816\n",
      "on\t0.022026117891073227\n",
      "at\t0.0199491735547781\n",
      "this\t0.013051669113337994\n",
      "\n",
      "== Topic #4 ==\n",
      "Labels: market, billion in, 45, cars, billion\n",
      "to\t0.045944344252347946\n",
      "for\t0.038988225162029266\n",
      "on\t0.020384658128023148\n",
      "and\t0.02006111666560173\n",
      "boston\t0.019414037466049194\n",
      "s\t0.017796335741877556\n",
      "year\t0.013266771100461483\n",
      "from\t0.013266771100461483\n",
      "more\t0.013266771100461483\n",
      "1\t0.013266771100461483\n",
      "\n",
      "== Topic #5 ==\n",
      "Labels: college football: https://apnews.com, AP college football: https://apnews, More AP college football: https, football: https://apnews, Collegefootball and            https://twitter.com\n",
      "to\t0.041002050042152405\n",
      "for\t0.034168656915426254\n",
      "image\t0.030410293489694595\n",
      "link\t0.029556119814515114\n",
      "the\t0.02750610187649727\n",
      "on\t0.019818540662527084\n",
      "a\t0.019476870074868202\n",
      "in\t0.017939357087016106\n",
      "state\t0.017939357087016106\n",
      "first\t0.01674351468682289\n",
      "\n",
      "== Topic #6 ==\n",
      "Labels: jury, testimony, prosecutor, kill, arrest\n",
      "the\t0.06157192215323448\n",
      "a\t0.05032608285546303\n",
      "in\t0.04367229342460632\n",
      "to\t0.04189170151948929\n",
      "was\t0.034300755709409714\n",
      "he\t0.03289502486586571\n",
      "said\t0.03083328902721405\n",
      "his\t0.02727210521697998\n",
      "s\t0.02146175131201744\n",
      "and\t0.020337168127298355\n",
      "\n",
      "== Topic #7 ==\n",
      "Labels: but I, those guys, right now, Stevens, ve got\n",
      "to\t0.040269579738378525\n",
      "it\t0.039572276175022125\n",
      "that\t0.037829022854566574\n",
      "s\t0.03482191264629364\n",
      "he\t0.034516844898462296\n",
      "i\t0.03303507715463638\n",
      "t\t0.023011375218629837\n",
      "said\t0.02179109677672386\n",
      "is\t0.015994781628251076\n",
      "we\t0.015733294188976288\n",
      "\n",
      "== Topic #8 ==\n",
      "Labels: supported, influence, attacks, Secretary of State, of the United States\n",
      "the\t0.05796092376112938\n",
      "to\t0.046463679522275925\n",
      "of\t0.03384041786193848\n",
      "s\t0.025899022817611694\n",
      "that\t0.024891532957553864\n",
      "in\t0.019617024809122086\n",
      "is\t0.018194684758782387\n",
      "for\t0.01647602580487728\n",
      "said\t0.015705591067671776\n",
      "be\t0.014698099344968796\n",
      "\n",
      "== Topic #9 ==\n",
      "Labels: attention, and her, she was, when she, she said\n",
      "she\t0.06931905448436737\n",
      "her\t0.06718622893095016\n",
      "to\t0.043298546224832535\n",
      "was\t0.04265869781374931\n",
      "for\t0.026235921308398247\n",
      "-\t0.024529660120606422\n",
      "that\t0.02324996143579483\n",
      "s\t0.020690567791461945\n",
      "had\t0.016851477324962616\n",
      "image\t0.012372536584734917\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ranking the candidates of labels for a specific topic\n",
    "labeler = tp.label.FoRelevance(mdl, cands, min_df=5, smoothing=1e-2, mu=0.25)\n",
    "for k in range(mdl.k):\n",
    "    print(\"== Topic #{} ==\".format(k))\n",
    "    print(\"Labels:\", ', '.join(label for label, score in labeler.get_topic_labels(k, top_n=5)))\n",
    "    for word, prob in mdl.get_topic_words(k, top_n=10):\n",
    "        print(word, prob, sep='\\t')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2b4542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
